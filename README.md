
---

## Otimiza√ß√£o do Spark

<p align="justify">Aqui est√£o algumas dicas para otimizar o Spark e economizar recursos:</p>

<p align="justify">O "bot√£o" que 90% dos usu√°rios Spark usa errado: spark.sql.shuffle.partitions.</p>

<p align="justify">(Salve ‚ôªÔ∏è porque o default "200" est√° custando caro para o seu cluster).</p>

<p align="justify">Quando o Spark faz um shuffle (um join, groupBy ou sort), ele precisa decidir em quantos "peda√ßos" (parti√ß√µes) ele vai quebrar o resultado. Esse n√∫mero √© controlado pelo spark.sql.shuffle.partitions. O valor padr√£o? 200. E aqui mora o problema. "200" √© um chute. √â um n√∫mero gen√©rico que n√£o faz ideia se voc√™ est√° processando 10MB ou 10TB.</p>

<p align="justify">Cen√°rio A: "Small Data" (Ex: 50MB) Voc√™ faz um groupBy. O Spark, obediente, cria 200 parti√ß√µes. Resultado: 195 parti√ß√µes vazias. Voc√™ gastou overhead de CPU e agendador para orquestrar 200 tarefas quando 5 seriam suficientes.</p>

### 1. Otimize o Shuffle Partitions

<p align="justify">O par√¢metro <code>spark.sql.shuffle.partitions</code> controla o n√∫mero de parti√ß√µes usadas durante as trocas de dados (joins e agrega√ß√µes). A recomenda√ß√£o √© manter cada parti√ß√£o entre 100 MB e 200 MB.</p>

| Tamanho Total dos Dados | `spark.sql.shuffle.partitions` | Justificativa |
| --- | --- | --- |
| **Pequeno** (< 1 GB) | 10 a 50 | Evita o overhead de muitas tarefas pequenas. |
| **M√©dio** (1 GB a 10 GB) | 50 a 200 | Mant√©m o paralelismo alinhado com executores m√©dios. |
| **Grande** (10 GB a 100 GB) | 200 a 1000 | Evita sobrecarga no Garbage Collection (GC). |
| **Muito Grande** (> 100 GB) | 1000+ | Necess√°rio para distribuir carga em clusters massivos. |

<p align="justify">Rela√ß√£o com o maxPartitionBytes: Enquanto o spark.sql.shuffle.partitions controla os dados durante as trocas (joins/agregados), o seu guia menciona o spark.sql.files.maxPartitionBytes. Este √∫ltimo controla a leitura inicial do disco.</p>

<p align="justify">Se voc√™ ler 10 GB de dados com maxPartitionBytes em 128 MB, ter√° inicialmente cerca de 80 parti√ß√µes. Se voc√™ n√£o ajustar o shuffle, o Spark usar√° o padr√£o de 200, o que pode ser excessivo para esse volume, gerando tarefas vazias.</p>

<p align="justify">Dica Extra: Sempre monitore a aba SQL no Spark UI. Se voc√™ notar que o "Shuffle Read Size" por tarefa est√° muito alto (ex: > 500 MB), aumente o n√∫mero de parti√ß√µes para evitar o uso excessivo de mem√≥ria do executor (spark.executor.memory).</p>

> **Dica:** No Spark 3.0+, habilite o AQE (`spark.sql.adaptive.enabled`) para que o Spark ajuste esse n√∫mero automaticamente.

### 2. Ajuste o tamanho dos blocos (block size)

<p align="justify">O par√¢metro <code>spark.sql.files.maxPartitionBytes</code> define o tamanho m√°ximo dos blocos lidos do disco, ajudando a reduzir o n√∫mero de tarefas iniciais.</p>

| Tamanho do arquivo | spark.sql.files.maxPartitionBytes | spark.sql.files.openCostInBytes |
| --- | --- | --- |
| **Pequeno** (< 100 MB) | 32 MB a 64 MB | 1 MB a 4 MB |
| **M√©dio** (100 MB a 1 GB) | 64 MB a 128 MB | 4 MB a 16 MB |
| **Grande** (1 GB a 10 GB) | 128 MB a 256 MB | 16 MB a 64 MB |
| **Muito grande** (> 10 GB) | 256 MB a 512 MB | 64 MB a 128 MB |

* <p align="justify"><b>Regra geral:</b> O tamanho dos blocos deve ser entre 1/10 e 1/5 do tamanho do arquivo.</p>
* <p align="justify"><b>Custo de abertura:</b> Deve ser entre 1/100 e 1/50 do tamanho do bloco.</p>

### 3. Use o cache de dados

* <p align="justify">Use <code>spark.cache</code> para armazenar dados acessados frequentemente em mem√≥ria.</p>
* <p align="justify">Utilize <code>cache()</code> ou <code>persist()</code> para evitar reprocessamento e reduzir leitura de disco.</p>

### 4. Otimize as jun√ß√µes (joins)

<p align="justify">O <b>broadcast</b> envia tabelas pequenas para todos os n√≥s, permitindo jun√ß√µes locais sem shuffle.</p>

| Categoria da Tabela | Tamanho | Ajuste do `spark.sql.autoBroadcastJoinThreshold` |
| --- | --- | --- |
| **Pequena** | < 10 MB | Transmitida automaticamente (padr√£o). |
| **M√©dia** | 10 MB a 100 MB | Aumente para 50 MB ou 100 MB. |
| **Grande** | > 100 MB | Geralmente n√£o √© transmitida automaticamente. |

### 5. Monitore e ajuste do paralelismo e garbage collection (GC)

<p align="justify">Ajuste o <code>spark.default.parallelism</code> e a mem√≥ria do executor para evitar falhas e lentid√£o no processamento.</p>

| Tamanho dos dados | spark.default.parallelism | spark.executor.memory |
| --- | --- | --- |
| **Pequeno** (< 100 MB) | 2-4 | 1-2 GB |
| **M√©dio** (100 MB a 1 GB) | 4-8 | 2-4 GB |
| **Grande** (1 GB a 10 GB) | 8-16 | 4-8 GB |
| **Muito grande** (> 10 GB) | 16-32 | 8-16 GB |

<p align="justify"><i>Tamanho dos dados pequeno (< 100 MB)</i></p>

```python
spark.conf.set("spark.default.parallelism", 2)
spark.conf.set("spark.sql.files.openCostInBytes", 1 * 1024 * 1024) # 1 MB

```

<p align="justify"><i>Tamanho dos dados m√©dio (100 MB a 1 GB)</i></p>

```python
spark.conf.set("spark.default.parallelism", 4)
spark.conf.set("spark.sql.files.openCostInBytes", 4 * 1024 * 1024) # 4 MB

```

<p align="justify"><i>Tamanho dos dados grande (1 GB a 10 GB)</i></p>

```python
spark.conf.set("spark.default.parallelism", 8)
spark.conf.set("spark.sql.files.openCostInBytes", 16 * 1024 * 1024) # 16 MB

```

<p align="justify"><i>Tamanho dos dados muito grande (> 10 GB)</i></p>

```python
spark.conf.set("spark.default.parallelism", 16)
spark.conf.set("spark.sql.files.openCostInBytes", 64 * 1024 * 1024) # 64 MB

```

<p align="justify">Lembre-se de que esses s√£o apenas exemplos e que o ajuste desses par√¢metros depende do seu ambiente de execu√ß√£o e do tamanho dos dados.</p>

<p align="justify"><b>Regra geral:</b></p>

* <p align="justify"><code>spark.default.parallelism</code>: 2-4 vezes o n√∫mero de n√∫cleos de CPU dispon√≠veis.</p>
* <p align="justify"><code>spark.sql.files.openCostInBytes</code>: 1-10% do tamanho do arquivo.</p>

<p align="justify">H√° v√°rias configura√ß√µes de mem√≥ria RAM do executor que voc√™ pode ajustar no Spark:</p>


<p align="justify">1. spark.executor.memory: define a mem√≥ria RAM total dispon√≠vel para cada executor</p>



<p align="justify">2. spark.executor.memoryOverhead: define a mem√≥ria adicional para o executor (por exemplo, para o sistema operacional e outros processos)</p>



<p align="justify">3. spark.memory.fraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados (padr√£o: 0,6)</p>



<p align="justify">4. spark.memory.storageFraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados em cache (padr√£o: 0,5)</p>



<p align="justify">5. spark.executor.pyspark.memory: define a mem√≥ria RAM dispon√≠vel para o Python worker (somente para PySpark)</p>



<p align="justify">6. spark.executor.pyspark.memoryOverhead: define a mem√≥ria adicional para o Python worker (somente para PySpark)</p>

```python
spark.conf.set("spark.executor.memory", "4g") # 4 GB de mem√≥ria RAM
spark.conf.set("spark.executor.memoryOverhead", "1g") # 1 GB de mem√≥ria adicional
spark.conf.set("spark.memory.fraction", 0.6) # 60% da mem√≥ria RAM para armazenamento de dados
spark.conf.set("spark.memory.storageFraction", 0.5) # 50% da mem√≥ria RAM para armazenamento de dados em cache

```

<p align="justify">Lembre-se de que o ajuste dessas configura√ß√µes depende do seu ambiente de execu√ß√£o e do tamanho dos dados.</p>

#### Configura√ß√µes de RAM do Executor:

* <p align="justify"><code>spark.executor.memoryOverhead</code>: Mem√≥ria para o SO e processos externos.</p>
* <p align="justify"><code>spark.memory.fraction</code>: Fra√ß√£o da RAM para armazenamento (padr√£o 0.6).</p>
* <p align="justify"><code>spark.memory.storageFraction</code>: Fra√ß√£o da RAM para cache (padr√£o 0.5).</p>

<p align="justify">Monitoramento do GC:</p>



<p align="justify">1. Acesse o Spark UI em <code>http://<driver-node>:4040</code></p>



<p align="justify">2. Clique em "Executors"</p>



<p align="justify">3. Verifique a coluna "GC Time" para cada executor</p>



<p align="justify">4. Se o tempo de GC for alto (> 10%), ajuste a mem√≥ria do executor</p>

### 6. Use o Spark SQL

<p align="justify">O Spark SQL (DataFrames e Datasets) √© mais eficiente que a RDD API devido ao otimizador Catalyst.</p>

<p align="justify">Lembre-se de monitorar o desempenho do seu aplicativo Spark e ajustar as configura√ß√µes conforme necess√°rio! üòä</p>

---

## Python

### 1. Estruturas de Dados: Tuplas vs. Listas

<p align="justify">As tuplas s√£o imut√°veis e possuem um tamanho fixo, o que torna sua aloca√ß√£o de mem√≥ria muito mais r√°pida que a das listas, que precisam de espa√ßo extra para redimensionamento din√¢mico.</p>

```python
# Lento: Lista (mut√°vel)
minha_lista = [1, 2, 3, 4, 5] 

# R√°pido: Tupla (imut√°vel)
minha_tupla = (1, 2, 3, 4, 5) 

```

<p align="justify">Resultado: Em testes, a cria√ß√£o de uma tupla pode ser cerca de 6 vezes mais r√°pida que a de uma lista.</p>

### 3. Buscas com Sets e Dicion√°rios

<p align="justify">Dicion√°rios e conjuntos (sets) utilizam tabelas de hash, permitindo que o Python encontre um item diretamente sem percorrer toda a estrutura. Isso resulta em uma busca de tempo constante, denotada como .</p>

```python
# Lento em listas grandes: O Python olha item por item
if 999999 in lista_de_um_milhao: 
    pass

# Instant√¢neo em Sets/Dicts: O Python vai direto ao endere√ßo
if 999999 in set_de_um_milhao: 
    pass

```

<p align="justify">Performance: Enquanto a busca em uma lista grande pode levar milissegundos, em um set ou dicion√°rio o tempo √© virtualmente zero.</p>

### 4. Vari√°veis Locais vs. Globais

<p align="justify">O Python utiliza a regra LEGB para buscar vari√°veis, come√ßando sempre pelo escopo local. Como o escopo local √© menor, a busca √© muito mais √°gil do que no escopo global.</p>

```python
# Menos eficiente
contador_global = 0
def teste_global():
    global contador_global
    for i in range(1000000):
        contador_global += 1

# Mais eficiente
def teste_local():
    contador_local = 0
    for i in range(1000000):
        contador_local += 1

```

<p align="justify">Nota: O uso de vari√°veis locais pode reduzir o tempo de execu√ß√£o em cerca de 35% em loops intensivos.</p>

### 5. Encapsulamento em Classes

<p align="justify">Manter vari√°veis restritas a fun√ß√µes e classes ajuda o interpretador a gerenciar menos nomes simultaneamente, melhorando a performance e a gest√£o de mem√≥ria.</p>

```python
class RetanguloEncapsulado:
    def __init__(self, largura, altura):
        self._largura = largura # Atributo protegido
        self._altura = altura

    def area(self):
        return self._largura * self._altura

```

<p align="justify">Benef√≠cio: Al√©m da performance, evita conflitos de nomes e garante que os dados n√£o sejam modificados acidentalmente por c√≥digo externo.</p>

### 6. List Comprehensions e Geradores

<p align="justify">As compreens√µes de lista s√£o otimizadas internamente, sendo mais r√°pidas que o uso do m√©todo .append() dentro de um loop for tradicional.</p>

```python
# R√°pido (List Comprehension)
quadrados = [x**2 for x in range(10)]

# Economiza mem√≥ria (Express√£o Geradora)
soma_quadrados = sum(x**2 for x in range(1000000))

```

<p align="justify">Compara√ß√£o: Express√µes geradoras s√£o mais r√°pidas e consomem muito menos mem√≥ria ao lidar com grandes volumes de dados.</p>

### 7. Fun√ß√µes Built-in e NumPy

<p align="justify">Sempre prefira as fun√ß√µes nativas do Python (escritas em C) ou bibliotecas especializadas como o NumPy para processamento num√©rico.</p>

```python
# Lento: Algoritmo manual (Bubble Sort)
def bubble_sort(arr): ... 

# Instant√¢neo: Fun√ß√£o nativa
sorted(meu_array)

# Soma com NumPy
import numpy as np
total = np.sum(array_numpy) # Muito mais r√°pido que sum() do Python para arrays gigantes

```

<p align="justify">Diferen√ßa: Em arrays grandes, o NumPy pode realizar opera√ß√µes em 0.008 segundos, enquanto uma fun√ß√£o Python customizada levaria mais de 1 segundo.</p>

---


